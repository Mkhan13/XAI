{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qisUgswkVmwq"
      },
      "source": [
        "# AIPI 590 - XAI | Human-AI Interaction\n",
        "### A code tutorial explaining LIME in code\n",
        "### Mariam Khan\n",
        "\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mkhan13/XAI/blob/main/human_ai_interaction.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LIME\n",
        "\n",
        "LIME (Local Interpretable Model-agnostic Explanations) helps us understand why a machine learning model made a specific prediction by creating a local, interpretable model around one specific instance. From the explainable AI lecture, the LIME Process is:\n",
        "\n",
        "\n",
        "1. Select instance of interest\n",
        "2. Perturb your dataset and get black box predictions for perturbed samples\n",
        "3. Generate a new dataset consisting of perturbed samples (variations of your data) and the corresponding predictions\n",
        "4. Train an interpretable model, weighted by the proximity of sampled instances to the instance of interest\n",
        "5. Interpret the local model to explain prediction\n"
      ],
      "metadata": {
        "id": "6782GM6gTldM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Hi7Qfe9N4_FU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Select Instance of Interest"
      ],
      "metadata": {
        "id": "A2L88xmu2MfA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmYSXjFs2Lu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Perturb Your Dataset and Get Black-Box Predictions for Perturbed Samples"
      ],
      "metadata": {
        "id": "A4ZxWRDV2mwB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rbgu8sUl1Tz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Generate a New Dataset of Perturbed Samples and Corresponding Predictions"
      ],
      "metadata": {
        "id": "bAPRsu694uQO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rWPInsKt4uWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Train an Interpretable Model Weighted by Proximity"
      ],
      "metadata": {
        "id": "YsulA4ih2nsl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u7xgHVJC2n1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Interpret the Local Model to Explain the Prediction"
      ],
      "metadata": {
        "id": "GaetdMjb2oBx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6vjWsOjV2oSd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}